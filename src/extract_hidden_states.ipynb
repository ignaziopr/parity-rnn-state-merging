{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "417b8cd3",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0c784ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "from collections import defaultdict\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from hopcroft import hopcroft_minimize\n",
    "\n",
    "# Add project root (one level up) to Python path \n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from models.parity_rnn import ParityRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61de824b",
   "metadata": {},
   "source": [
    "Load Trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bd901f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4z/s244bv2s5f7g8mctfr_jt3sc0000gn/T/ipykernel_5261/1250009842.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('../models/checkpoints/parity_rnn_checkpoint.pt', map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ParityRNN(\n",
       "  (rnn): RNN(2, 100, batch_first=True)\n",
       "  (readout): Linear(in_features=100, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ParityRNN(input_size=2, hidden_size=100, output_size=2).to(device)\n",
    "checkpoint = torch.load('../models/checkpoints/parity_rnn_checkpoint.pt', map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state\"])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28db1737",
   "metadata": {},
   "source": [
    "# --- 2. Load validation sequences ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfbb13cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = np.load('../data/validation/val_data_50.npz', allow_pickle=True)\n",
    "val_seqs = val['X']  # shape (100,), each an array of bits\n",
    "val_labels = val['Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33927828",
   "metadata": {},
   "source": [
    "# --- 3. Collect Hidden State Trajectories ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6da1e76",
   "metadata": {},
   "source": [
    "### 3a. Full trajectories for DFA extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab72af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "H_all, X_all, idx_map = [], [], []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for seq_i, seq in enumerate(val_seqs):\n",
    "        # build one-hot batch\n",
    "        x = torch.zeros(len(seq), 2, device=device)\n",
    "        for t, b in enumerate(seq):\n",
    "            x[t, b] = 1.0\n",
    "        out, _ = model.rnn(x.unsqueeze(0))       # [1, T, H]\n",
    "        h_seq = out.squeeze(0).cpu().numpy()     # (T, H)\n",
    "        H_all.append(h_seq)\n",
    "        X_all.append(list(seq))\n",
    "        # record index map\n",
    "        for t in range(h_seq.shape[0]):\n",
    "            idx_map.append((seq_i, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde0a166",
   "metadata": {},
   "source": [
    "### 3b. Final hidden states for clustering parities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63d7d088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 100 sequences of trajectories and 100 final hidden-state vectors.\n",
      "Collected 100 final hidden-state vectors.\n"
     ]
    }
   ],
   "source": [
    "H_finals, parities = [], []\n",
    "for seq_i, label in enumerate(val_labels):\n",
    "    h_seq = H_all[seq_i]              # full trajectory for sequence i\n",
    "    H_finals.append(h_seq[-1])        # last hidden state\n",
    "    parities.append(int(label))\n",
    "H_finals = np.stack(H_finals)         # (N, H)\n",
    "parities  = np.array(parities) \n",
    "print(f\"Collected {len(H_all)} sequences of trajectories and {H_finals.shape[0]} final hidden-state vectors.\") \n",
    "print(f\"Collected {H_finals.shape[0]} final hidden-state vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5034b076",
   "metadata": {},
   "source": [
    "# --- 4. Cluster Hidden States with DBSCAN ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f4f59dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA explained variance ratio sum: 0.952\n"
     ]
    }
   ],
   "source": [
    "# 3.1 Flatten all hidden states (order = idx_map order)\n",
    "flat_H = np.vstack(H_all)          # shape [N_tot, H]\n",
    "\n",
    "# 3.2 PCA to lower dimension (tune n_components if you like)\n",
    "pca = PCA(n_components=8, random_state=42)\n",
    "flat_H_p = pca.fit_transform(flat_H)\n",
    "\n",
    "print(f\"PCA explained variance ratio sum: {pca.explained_variance_ratio_.sum():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ce4c502d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBSCAN clustering over all states with eps=0.08 gave clusters={0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}\n"
     ]
    }
   ],
   "source": [
    "# 3.3 DBSCAN with an adaptive eps search until we get a reasonable #clusters\n",
    "\n",
    "eps = 0.08\n",
    "min_samples = 5\n",
    "db = DBSCAN(eps=eps, min_samples=min_samples).fit(flat_H_p)\n",
    "all_labels = db.labels_\n",
    "unique = set(all_labels)\n",
    "# we expect at least the two final-state parity clusters plus noise\n",
    "if len(unique) >= 3:\n",
    "    print(f\"DBSCAN clustering over all states with eps={eps} gave clusters={unique}\")\n",
    "else:\n",
    "    raise ValueError(\"DBSCAN failed to find enough clusters; adjust eps_values/min_samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a5c3d827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster→parity: 2=even, 3=odd\n"
     ]
    }
   ],
   "source": [
    "# 3.4 Rebuild per-sequence labels using idx_map\n",
    "labels_per_seq = [[] for _ in range(len(H_all))]\n",
    "for (seq_i, t), lbl in zip(idx_map, all_labels):\n",
    "    labels_per_seq[seq_i].append(lbl)\n",
    "\n",
    "final_labels = np.array([seq_lbls[-1] for seq_lbls in labels_per_seq])  # label at last timestep\n",
    "\n",
    "# Identify final-time non-noise clusters\n",
    "clusters_final = set(final_labels) - {-1}\n",
    "\n",
    "# Map each final cluster to parity by ground truth\n",
    "means = {}\n",
    "for c in clusters_final:\n",
    "    idxs = np.where(final_labels == c)[0]\n",
    "    means[c] = val_labels[idxs].mean()  # assuming val_labels are 0/1 parity\n",
    "odd_cluster  = max(means, key=means.get)\n",
    "even_cluster = min(means, key=means.get)\n",
    "print(f\"Cluster→parity: {even_cluster}=even, {odd_cluster}=odd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12529416",
   "metadata": {},
   "source": [
    "# --- 6. Build the Raw DFA Transitions ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4d6c6f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw DFA transitions:\n",
      "  0 --0--> 0\n",
      "  0 --1--> 21\n",
      "  1 --0--> 1\n",
      "  1 --1--> 5\n",
      "  2 --0--> 13\n",
      "  2 --1--> 2\n",
      "  3 --0--> 3\n",
      "  3 --1--> 4\n",
      "  4 --0--> 4\n",
      "  4 --1--> 2\n",
      "  5 --0--> 5\n",
      "  5 --1--> 9\n",
      "  6 --0--> 7\n",
      "  6 --1--> 6\n",
      "  7 --0--> 7\n",
      "  7 --1--> 8\n",
      "  8 --0--> 8\n",
      "  8 --1--> 9\n",
      "  9 --0--> 9\n",
      "  9 --1--> 2\n",
      "  10 --0--> 12\n",
      "  10 --1--> 10\n",
      "  11 --0--> 11\n",
      "  11 --1--> 11\n",
      "  12 --0--> 12\n",
      "  12 --1--> 17\n",
      "  13 --0--> 14\n",
      "  13 --1--> 13\n",
      "  14 --0--> 22\n",
      "  14 --1--> 14\n",
      "  15 --0--> 15\n",
      "  15 --1--> 16\n",
      "  16 --0--> 10\n",
      "  16 --1--> 16\n",
      "  17 --0--> 3\n",
      "  17 --1--> 17\n",
      "  18 --0--> 20\n",
      "  18 --1--> 18\n",
      "  19 --0--> 19\n",
      "  19 --1--> 4\n",
      "  20 --0--> 22\n",
      "  20 --1--> 20\n",
      "  21 --0--> 10\n",
      "  21 --1--> 21\n",
      "  22 --0--> 22\n",
      "  22 --1--> 22\n"
     ]
    }
   ],
   "source": [
    "Sigma = [0, 1]\n",
    "raw_Q = sorted(unique - {-1})  # drop noise label\n",
    "\n",
    "# Initialize count dict for majority vote\n",
    "from collections import defaultdict\n",
    "trans_counts = {(q,b): defaultdict(int) for q in raw_Q for b in Sigma}\n",
    "\n",
    "# Fill counts by walking trajectories\n",
    "for (seq_i, t), lbl in zip(idx_map, all_labels):\n",
    "    if lbl not in raw_Q:          # skip noise\n",
    "        continue\n",
    "    if t + 1 >= len(X_all[seq_i]):\n",
    "        continue                  # no next state\n",
    "    bit = X_all[seq_i][t]\n",
    "    nxt = all_labels[idx_map.index((seq_i, t+1))]\n",
    "    if nxt in raw_Q:\n",
    "        trans_counts[(lbl, bit)][nxt] += 1\n",
    "\n",
    "# Majority vote to decide single next state\n",
    "delta = {}\n",
    "for key, cnts in trans_counts.items():\n",
    "    if cnts:  # if we saw anything\n",
    "        delta[key] = max(cnts, key=cnts.get)\n",
    "    else:\n",
    "        # default self-loop if never observed\n",
    "        q,_ = key\n",
    "        delta[key] = q\n",
    "\n",
    "print(\"Raw DFA transitions:\")\n",
    "for (q,b), nxt in delta.items():\n",
    "    print(f\"  {q} --{b}--> {nxt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c23376e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Hopcroft Minimization ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "081e0ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial states (non-noise): 23\n",
      "Reachable states: 11\n",
      "Total states (after merge step): 11 | Output classes: 2\n",
      "Minimized to 2 states: [0, 1]\n",
      "Outputs per minimized state: {0: 0, 1: 1}\n",
      "Merged blocks (old->new):\n",
      "  0: [2]\n",
      "  1: [3]\n"
     ]
    }
   ],
   "source": [
    "# ========= STEP 6: Build reps, totalize δ, prune, (optional) merge unstable, minimize =========\n",
    "\n",
    "# 6.0 Fast index (seq_i,t) -> flat idx\n",
    "idx2flat = {pair: i for i, pair in enumerate(idx_map)}\n",
    "\n",
    "# 6.1 Initial state set Q (all non-noise clusters)\n",
    "Q = sorted(unique - {-1})\n",
    "print(f\"Initial states (non-noise): {len(Q)}\")\n",
    "\n",
    "# 6.2 representative_hidden for every q in Q\n",
    "representative_hidden = {}\n",
    "for (seq_i, t), lbl in zip(idx_map, all_labels):\n",
    "    if lbl in Q and lbl not in representative_hidden:\n",
    "        representative_hidden[lbl] = torch.from_numpy(H_all[seq_i][t]).to(device)\n",
    "    if len(representative_hidden) == len(Q):\n",
    "        break\n",
    "# fill any missing with a dummy\n",
    "if len(representative_hidden) < len(Q):\n",
    "    any_rep = next(iter(representative_hidden.values()))\n",
    "    for q in Q:\n",
    "        representative_hidden.setdefault(q, any_rep)\n",
    "\n",
    "# 6.3 Outputs & probabilities (Moore machine)\n",
    "outputs_full = {}\n",
    "probs_full   = {}\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for q, h in representative_hidden.items():\n",
    "        logits = model.readout(h.unsqueeze(0))[0]          # [2]\n",
    "        probs = torch.softmax(logits, dim=0)                # [2]\n",
    "        outputs_full[q] = int(torch.argmax(logits).item()) # 0/1\n",
    "        probs_full[q]   = probs[1].item()                   # P(odd)\n",
    "\n",
    "# 6.4 Ensure δ is total & closed by adding a sink if needed\n",
    "Sigma = [0, 1]\n",
    "sink = max(Q) + 1\n",
    "need_sink = False\n",
    "for q in Q:\n",
    "    for a in Sigma:\n",
    "        if (q, a) not in delta:\n",
    "            delta[(q, a)] = sink\n",
    "            need_sink = True\n",
    "\n",
    "if need_sink:\n",
    "    Q.append(sink)\n",
    "    # give sink a rep & output\n",
    "    rep_any = next(iter(representative_hidden.values()))\n",
    "    representative_hidden[sink] = rep_any\n",
    "    outputs_full[sink] = 0\n",
    "    probs_full[sink]   = 0.0\n",
    "\n",
    "# paranoia: ensure every target is in Q\n",
    "targets = {delta[(q,a)] for q in Q for a in Sigma}\n",
    "missing = targets - set(Q)\n",
    "if missing:\n",
    "    rep_any = next(iter(representative_hidden.values()))\n",
    "    for m in missing:\n",
    "        Q.append(m)\n",
    "        representative_hidden[m] = rep_any\n",
    "        outputs_full[m] = 0\n",
    "        probs_full[m]   = 0.0\n",
    "\n",
    "# 6.5 Reachability prune\n",
    "start_lbl = all_labels[idx2flat[(0, 0)]]\n",
    "reachable = {start_lbl}\n",
    "frontier = {start_lbl}\n",
    "while frontier:\n",
    "    nxt = set()\n",
    "    for q in frontier:\n",
    "        for a in Sigma:\n",
    "            q2 = delta[(q, a)]\n",
    "            if q2 not in reachable:\n",
    "                reachable.add(q2)\n",
    "                nxt.add(q2)\n",
    "    frontier = nxt\n",
    "\n",
    "Q = sorted(reachable)\n",
    "delta = {(q,a): delta[(q,a)] for q in Q for a in Sigma}\n",
    "outputs = {q: outputs_full[q] for q in Q}\n",
    "probs   = {q: probs_full[q]   for q in Q}\n",
    "representative_hidden = {q: representative_hidden[q] for q in Q}\n",
    "\n",
    "print(f\"Reachable states: {len(Q)}\")\n",
    "\n",
    "# (Optionally you could also re-run reachability / drop unstable from Q,\n",
    "# but it's okay to leave them; Hopcroft will merge them if truly equivalent.)\n",
    "\n",
    "# assume even_cluster, odd_cluster from final labels\n",
    "cent_even = representative_hidden[even_cluster]\n",
    "cent_odd  = representative_hidden[odd_cluster]\n",
    "\n",
    "def to_parity_block(q):\n",
    "    h = representative_hidden[q]\n",
    "    return even_cluster if torch.norm(h-cent_even) < torch.norm(h-cent_odd) else odd_cluster\n",
    "\n",
    "# coarsen transitions\n",
    "delta_par = {}\n",
    "for (q,a), qn in delta.items():\n",
    "    delta_par[(q,a)] = to_parity_block(qn)\n",
    "\n",
    "# parity outputs\n",
    "outputs_par = {even_cluster:0, odd_cluster:1}\n",
    "Q_par = [even_cluster, odd_cluster]\n",
    "Sigma = [0,1]\n",
    "for q in Q_par:\n",
    "    for a in Sigma:\n",
    "        delta_par.setdefault((q,a), q)\n",
    "\n",
    "print(f\"Total states (after merge step): {len(Q)} | Output classes: {len(set(outputs.values()))}\")\n",
    "\n",
    "# 6.7 Minimize (Moore version)\n",
    "min_states, Sigma, min_delta, min_outputs, state_map = hopcroft_minimize(\n",
    "    Q_par, Sigma, delta_par, outputs_par\n",
    ")\n",
    "print(f\"Minimized to {len(min_states)} states: {min_states}\")\n",
    "print(\"Outputs per minimized state:\", min_outputs)\n",
    "\n",
    "# Debug: show merged blocks\n",
    "blocks = {i: [] for i in min_states}\n",
    "for q_old, q_new in state_map.items():\n",
    "    blocks[q_new].append(q_old)\n",
    "print(\"Merged blocks (old->new):\")\n",
    "for new_id, olds in blocks.items():\n",
    "    print(f\"  {new_id}: {sorted(olds)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c10b6c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "872590d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final minimized DFA transitions (simulated & voted):\n",
      "  even (0) --0--> even (0)\n",
      "  even (0) --1--> odd (1)\n",
      "  odd (1) --0--> odd (1)\n",
      "  odd (1) --1--> even (0)\n"
     ]
    }
   ],
   "source": [
    "# Assume: min_states, state_map, blocks, Sigma, representative_hidden, model are defined\n",
    "Wxh = model.rnn.weight_ih_l0\n",
    "Whh = model.rnn.weight_hh_l0\n",
    "b   = model.rnn.bias_ih_l0 + model.rnn.bias_hh_l0\n",
    "act = torch.nn.ReLU()\n",
    "\n",
    "def step(h, bit):\n",
    "    x = torch.zeros(2, device=h.device); x[bit] = 1.0\n",
    "    return act(Wxh @ x + Whh @ h + b)\n",
    "\n",
    "# Representative hidden per minimized block\n",
    "rep_block = {}\n",
    "for s, olds in blocks.items():\n",
    "    Hs = torch.stack([representative_hidden[q] for q in olds], 0)\n",
    "    rep_block[s] = Hs.median(0).values  # median is more robust than mean\n",
    "\n",
    "# Map output→state (Moore)\n",
    "block_with_output = {}\n",
    "for s in min_states:\n",
    "    block_with_output[min_outputs[s]] = s\n",
    "\n",
    "min_delta_sim = {}\n",
    "for s, olds in blocks.items():\n",
    "    for a in Sigma:\n",
    "        votes = []\n",
    "        for q_old in olds:\n",
    "            h1 = step(representative_hidden[q_old], a)\n",
    "            # First try output-based routing\n",
    "            with torch.no_grad():\n",
    "                logits = model.readout(h1.unsqueeze(0))[0]\n",
    "                out = int(torch.argmax(logits))\n",
    "            if out in block_with_output:\n",
    "                nxt = block_with_output[out]\n",
    "            else:\n",
    "                # fallback: nearest centroid\n",
    "                dmin, nxt = 1e9, None\n",
    "                for s2, hb in rep_block.items():\n",
    "                    d = torch.norm(h1 - hb).item()\n",
    "                    if d < dmin:\n",
    "                        dmin, nxt = d, s2\n",
    "            votes.append(nxt)\n",
    "        min_delta_sim[(s,a)] = max(set(votes), key=votes.count)\n",
    "\n",
    "names = {s: (\"odd\" if min_outputs[s]==1 else \"even\") for s in min_states}\n",
    "print(\"Final minimized DFA transitions (simulated & voted):\")\n",
    "for s in min_states:\n",
    "    for a in Sigma:\n",
    "        nxt = min_delta_sim[(s,a)]\n",
    "        print(f\"  {names[s]} ({s}) --{a}--> {names[nxt]} ({nxt})\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8db2c035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON to dfas/minimal_dfa.json\n",
      "Saved DOT to dfas/minimal_dfa.dot\n",
      "Rendered to minimal_dfa.png\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# ---------- 1) Choose which minimized delta to export ----------\n",
    "# Use the simulated & voted one you just fixed:\n",
    "final_delta = min_delta_sim          # dict[(s,a)] = s_next\n",
    "states      = list(min_states)       # [0, 1]\n",
    "alphabet    = [0, 1]\n",
    "outputs     = {int(s): int(min_outputs[s]) for s in states}\n",
    "\n",
    "# Figure out which minimized state is the start (map original start cluster)\n",
    "orig_start = all_labels[idx2flat[(0,0)]]\n",
    "start_state = state_map[even_cluster]   # minimized id\n",
    "\n",
    "# ---------- 2) Save to JSON ----------\n",
    "dfa_json = {\n",
    "    \"states\": states,\n",
    "    \"alphabet\": alphabet,\n",
    "    \"start\": int(start_state),\n",
    "    \"transitions\": {f\"{s},{a}\": int(final_delta[(s,a)]) for s in states for a in alphabet},\n",
    "    \"outputs\": outputs                # Moore machine outputs\n",
    "}\n",
    "with open(\"dfas/minimal_dfa.json\", \"w\") as f:\n",
    "    json.dump(dfa_json, f, indent=2)\n",
    "print(\"Saved JSON to dfas/minimal_dfa.json\")\n",
    "\n",
    "# ---------- 3) Make a Graphviz DOT (looks like the paper’s figure) ----------\n",
    "# Helpers to label states as even/odd\n",
    "name = {s: (\"even\" if outputs[s]==0 else \"odd\") for s in states}\n",
    "\n",
    "dot_lines = [\n",
    "    \"digraph DFA {\",\n",
    "    \"  rankdir=LR;\",\n",
    "    '  node [shape=circle, style=filled, fillcolor=\"#cfeeee\", fontsize=18];'\n",
    "]\n",
    "\n",
    "# Invisible start node arrow\n",
    "dot_lines.append('  __start [shape=point, width=0];')\n",
    "dot_lines.append(f'  __start -> {start_state};')\n",
    "\n",
    "# State nodes (doublecircle optional if you want to highlight something)\n",
    "for s in states:\n",
    "    lab = f\"{name[s]}\\\\n({outputs[s]})\"\n",
    "    dot_lines.append(f'  {s} [label=\"{lab}\"];')\n",
    "\n",
    "# Edges\n",
    "for s in states:\n",
    "    for a in alphabet:\n",
    "        nxt = final_delta[(s,a)]\n",
    "        # put label \"0\" or \"1\" on the edge\n",
    "        dot_lines.append(f'  {s} -> {nxt} [label=\"{a}\", fontsize=18];')\n",
    "\n",
    "dot_lines.append(\"}\")\n",
    "dot_str = \"\\n\".join(dot_lines)\n",
    "with open(\"dfas/minimal_dfa.dot\", \"w\") as f:\n",
    "    f.write(dot_str)\n",
    "print(\"Saved DOT to dfas/minimal_dfa.dot\")\n",
    "\n",
    "# If graphviz is installed in your environment, auto-render to PNG:\n",
    "try:\n",
    "    import graphviz\n",
    "    g = graphviz.Source(dot_str)\n",
    "    g.render(\"dfas/minimal_dfa\", format=\"png\", cleanup=True)\n",
    "    print(\"Rendered to minimal_dfa.png\")\n",
    "except Exception as e:\n",
    "    print(\"Graphviz python package not available; run:\\n  dot -Tpng minimal_dfa.dot -o minimal_dfa.png\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
